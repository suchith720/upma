# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_ngame-for-msmarco-inference.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/00_ngame-for-msmarco-inference.ipynb 3
import os, torch,json, torch.multiprocessing as mp, joblib, numpy as np, scipy.sparse as sp, argparse
from tqdm.auto import tqdm

from xcai.basics import *
from xcai.analysis import *
from xcai.misc import *

import xclib.evaluation.xc_metrics as xm


def additional_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--num_examples', type=int, default=20)
    return parser.parse_known_args()[0]


def _ndcg(eval_flags, n, k=5):
    _cumsum = 0
    _dcg = np.cumsum(np.multiply(
        eval_flags, 1/np.log2(np.arange(k)+2)),
        axis=-1)
    ndcg = []
    for _k in range(k):
        _cumsum += 1/np.log2(_k+1+1)
        ndcg.append(np.multiply(_dcg[:, _k].reshape(-1, 1), 1/np.minimum(n, _cumsum)))
    return np.hstack(ndcg)


def ndcg(X, true_labels, k=5, sorted=False, use_cython=False):
    indices, true_labels, _, _ = xm._setup_metric(
        X, true_labels, k=k, sorted=sorted, use_cython=use_cython)
    eval_flags = xm._eval_flags(indices, true_labels, None)
    _total_pos = np.asarray(
        true_labels.sum(axis=1),
        dtype=np.int32)
    _max_pos = max(np.max(_total_pos), k)
    _cumsum = np.cumsum(1/np.log2(np.arange(1, _max_pos+1)+1))
    n = _cumsum[_total_pos - 1]
    return _ndcg(eval_flags, n, k)


# %% ../nbs/00_ngame-for-msmarco-inference.ipynb 20
if __name__ == '__main__':
    output_dir = "/home/aiscuser/scratch1/category_vs_substring/"

    pickle_dir = "/home/aiscuser/scratch1/datasets/processed/"
    extra_args = additional_args()

    # Metadata information
    info_file = "/data/datasets/beir/msmarco/XC/substring/raw_data/substring.raw.csv"
    sub_dict = {
        "info": Info.from_txt(info_file, info_column_names=["identifier", "input_text"]),
        "linker_dir": "/data/outputs/upma/00_msmarco-gpt-concept-substring-linker-with-ngame-loss-001/predictions/",
        "output_dir": "/data/outputs/upma/03_upma-with-ngame-gpt-substring-linker-for-msmarco-002/predictions/",
    }

    info_file = "/data/datasets/beir/msmarco/XC/raw_data/category-gpt-linker_conflated-001_conflated-001.raw.csv"
    cat_dict = {
        "info": Info.from_txt(info_file, info_column_names=["identifier", "input_text"]),
        "linker_dir": "/data/outputs/mogicX/47_msmarco-gpt-category-linker-007/predictions/",
        "output_dir": "/data/outputs/upma/04_upma-with-ngame-gpt-category-linker-for-msmarco-001/predictions/",
    }

    for dataset in tqdm(BEIR_DATASETS):
        # basic dataset
        config_file = f"/data/datasets/beir/{dataset}/XC/configs/data.json"
        config_key, fname = get_config_key(config_file)

        dataset = dataset.replace("/", "-")
        pkl_file = get_pkl_file(pickle_dir, f"{dataset}_{fname}_distilbert-base-uncased", use_sxc_sampler=True, use_only_test=True)
        block = build_block(pkl_file, config_file, use_sxc=True, config_key=config_key, only_test=True, main_oversample=True,
                            return_scores=True, n_slbl_samples=1)

        # Metadata predictions
        data_sub = sp.load_npz(f"{sub_dict['linker_dir']}/test_predictions_{dataset}.npz")
        data_cat = sp.load_npz(f"{cat_dict['linker_dir']}/test_predictions_{dataset}.npz")

        # Label predictions
        data_lbl_sub = sp.load_npz(f"{sub_dict['output_dir']}/test_predictions_{dataset}.npz")
        data_lbl_cat = sp.load_npz(f"{cat_dict['output_dir']}/test_predictions_{dataset}.npz")

        assert data_sub.shape[0] == data_cat.shape[0]
        assert data_lbl_sub.shape == data_lbl_cat.shape

        # Prediction scores
        sub_scores = ndcg(data_lbl_sub, block.test.dset.data.data_lbl, k=10)[:, -1]
        cat_scores = ndcg(data_lbl_cat, block.test.dset.data.data_lbl, k=10)[:, -1]

        print(f"{dataset} -> (category: {cat_scores.mean():.4f}; substring: {sub_scores.mean():.4f})")

        # Dataset
        meta_kwargs = {
            "sub_meta": SMetaXCDataset(prefix="sub", data_meta=data_sub, meta_info=sub_dict["info"], return_scores=True),
            "cat_meta": SMetaXCDataset(prefix="cat", data_meta=data_cat, meta_info=cat_dict["info"], return_scores=True),
        }
        test_dset = SXCDataset(block.test.dset.data, **meta_kwargs)

        # Display predictions
        example_dir = f"{output_dir}/examples"
        os.makedirs(example_dir, exist_ok=True)

        np.random.seed(1000)
        idxs = np.random.permutation(data_sub.shape[0])[:extra_args.num_examples]

        disp_block = TextDataset(test_dset, pattern=".*(_text|_scores)$", combine_info=True, sort_by="scores")

        items = [disp_block.combine_by_prefix(disp_block[idx]) if disp_block.combine_info else disp_block[idx] for idx in idxs]
        for idx, item in zip(idxs, items): item.update({"data_substring_metric":sub_scores[idx], "data_category_metric":cat_scores[idx]})

        with open(f"{example_dir}/test_examples_{dataset}.json", 'w') as file:
            json.dump(items, file, indent=4)

