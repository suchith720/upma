# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_ngame-for-msmarco-inference.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/00_ngame-for-msmarco-inference.ipynb 3
import os, torch,json, torch.multiprocessing as mp, joblib, numpy as np, scipy.sparse as sp, argparse
from tqdm.auto import tqdm

from xcai.basics import *
from xcai.analysis import *

DATASETS = [
    "msmarco",
    "arguana",
    "climate-fever",
    "dbpedia-entity",
    "fever",
    "fiqa",
    "hotpotqa",
    "nfcorpus",
    "nq",
    "quora",
    "scidocs",
    "scifact",
    "webis-touche2020",
    "trec-covid",
    "cqadupstack/android",
    "cqadupstack/english",
    "cqadupstack/gaming",
    "cqadupstack/gis",
    "cqadupstack/mathematica",
    "cqadupstack/physics",
    "cqadupstack/programmers",
    "cqadupstack/stats",
    "cqadupstack/tex",
    "cqadupstack/unix",
    "cqadupstack/webmasters",
    "cqadupstack/wordpress"
]

def additional_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--num_examples', type=int, default=20) 
    return parser.parse_known_args()[0]

# %% ../nbs/00_ngame-for-msmarco-inference.ipynb 20
if __name__ == '__main__':
    output_dir = "/data/outputs/upma/00_msmarco-gpt-concept-substring-linker-with-ngame-loss-001"
    pickle_dir = "/home/aiscuser/scratch1/datasets/processed/"
    mname = "distilbert-base-uncased"

    extra_args = additional_args()

    for dataset in tqdm(DATASETS):
        # test-data
        test_info = load_info(f"{pickle_dir}/beir/{dataset.replace('/', '-')}.joblib", 
                              f"/data/datasets/beir/{dataset}/XC/raw_data/test.raw.csv", 
                              mname, sequence_length=32)

        # meta-data
        meta_info = load_info(f"{pickle_dir}/concept-substring.joblib",
                              "/data/datasets/beir/msmarco/XC/concept_substrings/raw_data/concept-substring.raw.csv", 
                              mname, sequence_length=64)

        # test-meta matrix
        dataset = dataset.replace("/", "-")
        data_lbl = sp.load_npz(f"{output_dir}/predictions/test_predictions_{dataset}.npz")

        # dataset
        test_dset = SXCDataset(SMainXCDataset(data_info=test_info, data_lbl=data_lbl, lbl_info=meta_info, return_scores=True))


        # display
        example_dir = f"{output_dir}/examples"
        os.makedirs(example_dir, exist_ok=True)
        disp_block = TextDataset(test_dset, pattern=".*(_text|_scores)$", combine_info=True, sort_by="scores")

        np.random.seed(1000)
        idxs = np.random.permutation(data_lbl.shape[0])[:extra_args.num_examples]
        disp_block.dump(f"{example_dir}/test_examples_{dataset}.json", idxs)

