# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/42_entity-conflation.ipynb.

# %% auto 0
__all__ = ['show_conflated_labels', 'load_data', 'Filter', 'get_one_hop', 'normalize_matrix', 'compute_embed_similarity',
           'get_components', 'get_valid_components', 'get_conflated_info', 'get_id_to_cluster_idx_mapping',
           'get_conflated_matrix', 'cluster_length_stats', 'get_conflated_path', 'save_conflated_data', 'main',
           'parse_args']

# %% ../nbs/42_entity-conflation.ipynb 2
import scipy.sparse as sp, numpy as np, argparse, os, torch, pandas as pd
from tqdm.auto import tqdm
from termcolor import colored, COLORS
from scipy.sparse.csgraph import connected_components
from typing import List, Optional, Dict, Set, Tuple

from sugar.statistics_utils import matrix_stats
from sugar.core import load_raw_file, save_raw_file
from xclib.utils.sparse import retain_topk

from xcai.clustering.cluster import BalancedClusters, get_cluster_size

# %% ../nbs/42_entity-conflation.ipynb 5
def show_conflated_labels(idxs:List, components:Dict, lbl_ids2txt:Dict, fname:Optional[str]=None):
    file = fname if fname is None else open(fname, 'w')
    for i, idx in enumerate(idxs):
        txt = " || ".join([lbl_ids2txt[o] for o in components[idx]])
        if fname is None: print(f'{i+1:03d}. {txt}')
        else: file.write(f'{i+1:03d}. {txt}\n')
    if fname is not None: file.close()
        

# %% ../nbs/42_entity-conflation.ipynb 9
def load_data(pred_file:str, trn_file:str, tst_file:str, lbl_info_file:str, embed_file:Optional[str]=None,
              encoding:Optional[str]='utf-8'):
    pred_lbl, trn_lbl, tst_lbl = sp.load_npz(pred_file), sp.load_npz(trn_file), sp.load_npz(tst_file)
    lbl_ids, lbl_txt = load_raw_file(lbl_info_file, encoding=encoding)
    lbl_repr = None if embed_file is None else torch.load(embed_file)
    return pred_lbl, trn_lbl, tst_lbl, (lbl_ids, lbl_txt), lbl_repr
    

# %% ../nbs/42_entity-conflation.ipynb 15
class Filter:

    @staticmethod
    def by_length(components:Dict, min_thresh:Optional[int]=1, max_thresh:Optional[int]=100):
        cluster_len = np.array([len(components[idx]) for idx in sorted(components)])

        mask = None if min_thresh is None else np.where(cluster_len >= min_thresh, 1, 0)
        if max_thresh is not None:
            max_mask = np.where(cluster_len <= max_thresh, 1, 0)
            mask = max_mask if mask is None else np.logical_and(mask, max_mask)

        return set(range(len(components))) if mask is None else set(np.where(mask)[0]) 

    @staticmethod
    def topk(data_lbl:sp.csr_matrix, k:Optional[int]=3):
        return retain_topk(data_lbl, k=k)

    @staticmethod
    def lbl_freq_threhold(data_lbl:sp.csr_matrix, k:Optional[int]=300):
        lbl_freq = data_lbl.getnnz(axis=0)
        idx = np.where(lbl_freq > k)[0]

        data_lbl = data_lbl.tocsc()
        for i in idx:
            p,q = data_lbl.indptr[i], data_lbl.indptr[i+1]
            data_lbl.data[p:q] = 0
        data_lbl.eliminate_zeros()

        return data_lbl.tocsr()

    @staticmethod
    def threshold(data_lbl:sp.csr_matrix, t:int):
        idx = np.where(data_lbl.data < t)[0]
        data_lbl.data[idx] = 0
        data_lbl.eliminate_zeros()
        return data_lbl

    @staticmethod
    def difference(data_lbl:sp.csr_matrix, t:int):
        rowise_max = data_lbl.max(axis=1).toarray().ravel()
        scores = np.repeat(rowise_max, np.diff(data_lbl.indptr)) - data_lbl.data
        data_lbl.data[scores > t] = 0
        data_lbl.eliminate_zeros()
        return data_lbl
        

# %% ../nbs/42_entity-conflation.ipynb 17
def get_one_hop(data_lbl:sp.csr_matrix, batch_size:Optional[int]=1024):
    data_lbl = data_lbl.copy()
    data_lbl.data[:] = 1.0
    
    lbl_data = data_lbl.T.tocsr()
    lbl_lbl = [lbl_data[i:i+batch_size]@data_lbl for i in tqdm(range(0, lbl_data.shape[0], batch_size))]
    return sp.vstack(lbl_lbl)
    

# %% ../nbs/42_entity-conflation.ipynb 19
def normalize_matrix(lbl_lbl:sp.csr_matrix):
    lbl_lbl = lbl_lbl + sp.eye(lbl_lbl.shape[0])
    row_deg, col_deg = lbl_lbl.sum(axis=1), lbl_lbl.sum(axis=0)
    lbl_lbl = lbl_lbl.multiply(np.sqrt(1/row_deg)).multiply(np.sqrt(1/col_deg)).tocsr()
    return lbl_lbl
    

# %% ../nbs/42_entity-conflation.ipynb 20
def compute_embed_similarity(lbl_lbl:sp.csr_matrix, lbl_repr:torch.Tensor, batch_size:Optional[int]=1024):
    lbl_lbl = lbl_lbl.tocoo()
    scores = []
    for i in tqdm(range(0, lbl_lbl.nnz, batch_size)):
        row_idx, col_idx = lbl_lbl.row[i:i+batch_size], lbl_lbl.col[i:i+batch_size]
        sc = lbl_repr[row_idx].view(len(row_idx), 1, -1) @ lbl_repr[col_idx].view(len(col_idx), -1, 1)
        scores.append(sc.squeeze(1).squeeze(1))
    scores = torch.hstack(scores)
    lbl_lbl.data[:] = scores.numpy()
    return lbl_lbl.tocsr()
    

# %% ../nbs/42_entity-conflation.ipynb 21
def get_components(data_lbl:sp.csr_matrix, lbl_ids:List, lbl_repr:Optional[torch.Tensor]=None, 
                   score_thresh:Optional[float]=25, freq_thresh:Optional[float]=50, batch_size:Optional[int]=1024):
    lbl_lbl = get_one_hop(data_lbl, batch_size)
    lbl_lbl = normalize_matrix(lbl_lbl)
    lbl_lbl = Filter.threshold(lbl_lbl, t=np.percentile(lbl_lbl.data, q=freq_thresh))
    
    if lbl_repr is not None:
        lbl_lbl = compute_embed_similarity(lbl_lbl, lbl_repr, batch_size=batch_size)
        lbl_lbl = Filter.threshold(lbl_lbl, t=np.percentile(lbl_lbl.data, q=score_thresh))
    
    n_comp, clusters = connected_components(lbl_lbl, directed=False, return_labels=True)
    components = {}
    for idx,ids in zip(clusters, lbl_ids):
        components.setdefault(idx, []).append(ids)
    return components
    

# %% ../nbs/42_entity-conflation.ipynb 29
def get_valid_components(components:Dict, valid_cluster_idxs:Set, include_invalid:Optional[bool]=True):
    valid_components, lbl_ids2cluster = dict(), dict()

    curr_cluster_idx = 0
    for idx, cluster in components.items():
        if idx in valid_cluster_idxs:
            valid_components[curr_cluster_idx] = cluster
            for o in cluster: lbl_ids2cluster[o] = curr_cluster_idx
            curr_cluster_idx += 1
        else:
            if include_invalid:
                for o in cluster:
                    valid_components[curr_cluster_idx] = [o]
                    lbl_ids2cluster[o] = curr_cluster_idx
                    curr_cluster_idx += 1
                
    return valid_components, lbl_ids2cluster
    

# %% ../nbs/42_entity-conflation.ipynb 30
def _conflate_info_txt(txts:List, type:Optional[str]='max'):
    if type == "max":
        idx = int(np.argmax([len(o) for o in txts]))
        return txts[idx]
    elif type == "mid":
        idx = int(np.argsort([len(o) for o in txts])[(len(txts)-1)//2])
        return txts[idx]
    elif type == "concat":
        return " || ".join(txts)
    else:
        raise ValueError(f"Invalid type: {type}")

def _conflate_info(txts:List, reps:List, type:Optional[str]='max'):
    if type == "max":
        idx = int(np.argmax([len(o) for o in txts]))
        return txts[idx], reps[idx]
    elif type == "mid":
        idx = int(np.argsort([len(o) for o in txts])[(len(txts)-1)//2])
        return txts[idx], reps[idx]
    elif type == "concat":
        return " || ".join(txts), torch.vstack(reps).mean(dim=0)
    else:
        raise ValueError(f"Invalid type: {type}")
    
def get_conflated_info(components:Dict, lbl_ids2txt:Dict, lbl_embed:Optional[torch.Tensor]=None, lbl_ids2idx:Optional[Dict]=None, 
        type:Optional[str]="max"):
    lbl_txt, lbl_repr = list(), list()

    for i in tqdm(sorted(components), total=len(components)):
        cluster_txt = [lbl_ids2txt[o] for o in components[i]]
        if lbl_embed is not None:
            cluster_rep = [lbl_embed[lbl_ids2idx[o]] for o in components[i]]
            txt, rep = _conflate_info(cluster_txt, cluster_rep, type)
            lbl_repr.append(rep)
        else:
            txt = _conflate_info_txt(cluster_txt, type)
        lbl_txt.append(txt) 

    # lbl_txt = [_conflate_info_txt([lbl_ids2txt[o] for o in comonents[i]], type) for i in sorted(components)]
    return lbl_txt, torch.vstack(lbl_repr) if len(lbl_repr) else None
        

# %% ../nbs/42_entity-conflation.ipynb 31
def get_id_to_cluster_idx_mapping(lbl_ids2cluster_map:Dict, lbl_ids:List):
    return np.array([lbl_ids2cluster_map.get(o, -1) for o in lbl_ids])
    

# %% ../nbs/42_entity-conflation.ipynb 32
def get_conflated_matrix(data_lbl:sp.csr_matrix, lbl_ids2cluster:Dict, n_clusters:Optional[Tuple]=None):
    indices, data = list(), list()
    for idx in data_lbl.indices:
        i = lbl_ids2cluster[idx]

        if i == -1: 
            indices.append(0)
            data.append(0)
        else:
            indices.append(i)
            data.append(1)

    if n_clusters is not None:
        assert max(indices) < n_clusters
    
    matrix = (
        sp.csr_matrix((data, indices, data_lbl.indptr), dtype=np.float32) 
        if n_clusters is None else 
        sp.csr_matrix((data, indices, data_lbl.indptr), shape=(data_lbl.shape[0], n_clusters), dtype=np.float32)
    )
    matrix.eliminate_zeros()
    matrix.sum_duplicates()
    return matrix
    

# %% ../nbs/42_entity-conflation.ipynb 36
def cluster_length_stats(components):
    print(f'Number of components: {len(components)}')
    lengths = np.array([len(o) for o in components.values() if len(o) > 1])
    print(f'Number of clusters: {len(lengths)}', end='\n\n')
    with pd.option_context('display.precision', 3):
        print(pd.DataFrame(lengths).describe().T.to_dict())

def _matrix_stats(mat, label='matrix'):
    print(label)
    stats = matrix_stats(mat)
    with pd.option_context('display.precision', 3, "display.max_columns", None):
        print(pd.DataFrame([stats]).to_dict())

    print('- label frequency: ')
    lbl_freq = mat.getnnz(axis=0)
    with pd.option_context('display.precision', 3):
        print(pd.DataFrame(lbl_freq).describe().T.to_dict())

# %% ../nbs/42_entity-conflation.ipynb 44
def get_conflated_path(fname:str, output_dir:Optional[str]=None, output_prefix:Optional[str]=''):
    file_dir = os.path.dirname(fname) if output_dir is None else output_dir
    os.makedirs(file_dir, exist_ok=True)

    if len(output_prefix): output_prefix = '-' + output_prefix
    file_name, file_type = os.path.basename(fname).split('.', maxsplit=1)
    return f'{file_dir}/{file_name}_conflated{output_prefix}.{file_type}'
    

# %% ../nbs/42_entity-conflation.ipynb 45
def save_conflated_data(lbl_txt:List, lbl_file:str, trn_lbl:sp.csr_matrix, trn_file:str, tst_lbl:sp.csr_matrix, tst_file:str, 
        output_dir:Optional[str]=None, output_prefix:Optional[str]=''):
    lbl_file = get_conflated_path(lbl_file, output_dir if output_dir is None else f"{output_dir}/raw_data", output_prefix)
    trn_file = get_conflated_path(trn_file, output_dir, output_prefix)
    tst_file = get_conflated_path(tst_file, output_dir, output_prefix)

    save_raw_file(lbl_file, range(len(lbl_txt)), lbl_txt)
    sp.save_npz(trn_file, trn_lbl)
    sp.save_npz(tst_file, tst_lbl)
    
def prune_pred_lbl(pred_lbl:sp.csr_matrix, topk:Optional[int]=None, pred_lbl_freq:Optional[int]=None, diff_thresh:Optional[float]=None, 
        pred_score_thresh:Optional[float]=None):
    # Pick top-k predictions
    data_lbl = pred_lbl if topk is None else Filter.topk(pred_lbl, k=topk) 

    # Label frequency based thresholding
    if pred_lbl_freq is not None: data_lbl = Filter.lbl_freq_threhold(data_lbl, pred_lbl_freq)

    # Difference based thresholding
    if diff_thresh is not None: data_lbl = Filter.difference(data_lbl, t=diff_thresh)

    # Score thresholding
    if pred_score_thresh is not None: data_lbl = Filter.threshold(data_lbl, t=pred_score_thresh)

    return data_lbl

def prune_clusters(components:Dict, min_size_thresh:Optional[int]=None, max_size_thresh:Optional[int]=None, include_invalid:Optional[bool]=True):
    valid_cluster_idxs = Filter.by_length(components, min_thresh=min_size_thresh, max_thresh=max_size_thresh)
    valid_components, lbl_ids2cluster_map = get_valid_components(components, valid_cluster_idxs, include_invalid)
    return valid_components, lbl_ids2cluster_map


def prune_labels(lbl_txt:List, trn_lbl:sp.csr_matrix, tst_lbl:sp.csr_matrix, lbl_repr:Optional[torch.Tensor]=None, 
        lbl_freq_thresh:Optional[int]=300, lbl_cluster_sz:Optional[int]=2):
    assert lbl_repr.shape[0] == len(lbl_txt)

    def _map_matrix(mat, mapping):
        mat.indices[:] = [mapping.get(i, i) for i in mat.indices]
        mat.sum_duplicates()
        mat.eliminate_zeros()

    def _valid_info(txt, trn, tst, idx):
        txt = [txt[i] for i in idx]
        trn = trn[:, idx].tocsr()
        tst = tst[:, idx].tocsr()
        return txt, trn, tst

    if lbl_freq_thresh is not None:
        idx = np.where(trn_lbl.getnnz(axis=0) < lbl_freq_thresh)[0]
        lbl_txt, trn_lbl, tst_lbl = _valid_info(lbl_txt, trn_lbl, tst_lbl, idx)
        if lbl_repr is not None: lbl_repr = lbl_repr[idx]

    if lbl_repr is not None and lbl_cluster_sz is not None:
        idx = np.where(trn_lbl.getnnz(axis=0) == 1)[0]
        
        clusters = BalancedClusters.proc(lbl_repr[idx].half(), min_cluster_sz=lbl_cluster_sz)
        lbl_mapping = {idx[o]:idx[cluster[0]] for cluster in clusters for o in cluster}

        _map_matrix(trn_lbl, lbl_mapping)
        _map_matrix(tst_lbl, lbl_mapping)

        idx = np.where(trn_lbl.getnnz(axis=0) > 0)[0]
        lbl_txt, trn_lbl, tst_lbl = _valid_info(lbl_txt, trn_lbl, tst_lbl, idx)

    return lbl_txt, trn_lbl, tst_lbl


# %% ../nbs/42_entity-conflation.ipynb 48
def main(pred_file:str, trn_file:str, tst_file:str, lbl_info_file:str, embed_file:Optional[str]=None, output_dir:Optional[str]=None,
        topk:Optional[int]=None, pred_lbl_freq:Optional[int]=None, diff_thresh:Optional[float]=None, pred_score_thresh:Optional[float]=None, 
        batch_size:Optional[int]=1024, sim_score_thresh:Optional[float]=25, freq_thresh:Optional[float]=50, min_size_thresh:Optional[int]=None, 
        max_size_thresh:Optional[int]=None, conflated_lbl_freq:Optional[int]=None, lbl_cluster_sz:Optional[int]=None, print_stats:Optional[bool]=False, 
        type:Optional[str]="max", encoding:Optional[str]='latin-1', include_invalid:Optional[bool]=True, dont_save:Optional[bool]=False, 
        output_prefix:Optional[str]=''):
    # Load data
    pred_lbl, trn_lbl, tst_lbl, lbl_info, lbl_repr = load_data(pred_file, trn_file, tst_file, lbl_info_file, embed_file, encoding=encoding)
    lbl_ids, lbl_txt = lbl_info

    # Prune predictions
    data_lbl = prune_pred_lbl(pred_lbl, topk, pred_lbl_freq, diff_thresh, pred_score_thresh)

    # Get connected components
    components = get_components(data_lbl, lbl_ids, lbl_repr=lbl_repr, score_thresh=sim_score_thresh, 
                                freq_thresh=freq_thresh, batch_size=batch_size)

    # Prune clusters
    components, lbl_ids2cluster_map = prune_clusters(components, min_size_thresh, max_size_thresh, include_invalid=include_invalid)

    # Statistics
    if print_stats: cluster_length_stats(components)

    # Conflate label text
    assert lbl_repr.shape[0] == len(lbl_ids)
    lbl_ids2txt, lbl_ids2idx = {k:v for k,v in zip(lbl_ids, lbl_txt)}, {k:i for i,k in enumerate(lbl_ids)}
    conflated_lbl_txt, conflated_lbl_repr = get_conflated_info(components, lbl_ids2txt, lbl_embed=lbl_repr, lbl_ids2idx=lbl_ids2idx, type=type)

    # Conflate matrix
    lbl_ids2cluster = get_id_to_cluster_idx_mapping(lbl_ids2cluster_map, lbl_ids)
    conflated_trn_lbl = get_conflated_matrix(trn_lbl, lbl_ids2cluster)
    conflated_tst_lbl = get_conflated_matrix(tst_lbl, lbl_ids2cluster, n_clusters=conflated_trn_lbl.shape[1])

    # Prune labels
    lbl_txt, trn_lbl, tst_lbl = prune_labels(conflated_lbl_txt, conflated_trn_lbl, conflated_tst_lbl, conflated_lbl_repr, 
            lbl_freq_thresh=conflated_lbl_freq, lbl_cluster_sz=lbl_cluster_sz)

    # Statistics
    if print_stats:
        _matrix_stats(trn_lbl, 'conflated_trn_lbl')
        _matrix_stats(tst_lbl, 'conflated_tst_lbl')

    if not dont_save:
        save_conflated_data(lbl_txt, lbl_info_file, trn_lbl, trn_file, tst_lbl, tst_file, output_dir=output_dir, output_prefix=output_prefix)
    

# %% ../nbs/42_entity-conflation.ipynb 50
def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--pred_file', type=str, required=True)
    parser.add_argument('--trn_file', type=str, required=True)
    parser.add_argument('--tst_file', type=str, required=True)
    parser.add_argument('--lbl_info_file', type=str, required=True)
    parser.add_argument('--embed_file', type=str, default=None)
    parser.add_argument('--output_dir', type=str, default=None)

    parser.add_argument('--topk', type=int, default=None)
    parser.add_argument('--pred_lbl_freq', type=int, default=None)
    parser.add_argument('--diff_thresh', type=float, default=None)
    parser.add_argument('--pred_score_thresh', type=float, default=None)
    parser.add_argument('--batch_size', type=int, default=1024)
    
    parser.add_argument('--sim_score_thresh', type=float, default=25)
    parser.add_argument('--freq_thresh', type=float, default=50)

    parser.add_argument('--min_size_thresh', type=int, default=None)
    parser.add_argument('--max_size_thresh', type=int, default=None)
    parser.add_argument('--conflated_lbl_freq', type=int, default=None)
    parser.add_argument('--lbl_cluster_sz', type=int, default=None)

    parser.add_argument('--type', type=str, default='max')

    parser.add_argument('--print_stats', action='store_true')
    parser.add_argument('--encoding', type=str, default='latin-1')
    parser.add_argument('--exclude_invalid', action='store_true')
    parser.add_argument('--dont_save', action='store_true')
    parser.add_argument('--output_prefix', type=str, default='')
    
    return parser.parse_args()
    

# %% ../nbs/42_entity-conflation.ipynb 51
if __name__ == '__main__':
    args = parse_args()

    main(args.pred_file, args.trn_file, args.tst_file, args.lbl_info_file, args.embed_file, output_dir=args.output_dir,
         topk=args.topk, pred_lbl_freq=args.pred_lbl_freq, diff_thresh=args.diff_thresh, pred_score_thresh=args.pred_score_thresh,  
         batch_size=args.batch_size, sim_score_thresh=args.sim_score_thresh, freq_thresh=args.freq_thresh, 
         min_size_thresh=args.min_size_thresh, max_size_thresh=args.max_size_thresh, conflated_lbl_freq=args.conflated_lbl_freq, 
         lbl_cluster_sz=args.lbl_cluster_sz, print_stats=args.print_stats, type=args.type, encoding=args.encoding, 
         include_invalid=not args.exclude_invalid, dont_save=args.dont_save, output_prefix=args.output_prefix)

